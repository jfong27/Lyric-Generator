{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Markov Chain Model\n",
    "\n",
    "Now you'll build a more complex Markov chain that uses the last _two_ words (or bigram) to predict the next word. Now your dict `chain` should map a _tuple_ of words to a list of words that appear after it. So for example, one entry of this dict might be\n",
    "\n",
    "```\n",
    "chain = {\n",
    "    (\"it\", \"is\"): [\"the\", \"the\", \"not\", \"a\", \"a\", \"not\", \"the\"],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "As before, you should also include tags that indicate the beginning and end of a song, as well as line breaks. That is, a tuple might contain tags like `\"<START>\"`, `\"<END>\"`, and `\"<N>\"`, in addition to regular words. So if the song starts with the line \"Is this the real life?\" and ends with the line \"Nothing really matters to me.\", you would have a dictionary that looks like\n",
    "```\n",
    "chain = {\n",
    "    (None, \"<START>\"): [\"Is\", ...],\n",
    "    (\"<START>\", \"Is\"): [\"this\", ...],\n",
    "    (\"Is\", \"this\"): [\"the\", ...],\n",
    "    (\"this\", \"the\"): [\"real\", ...],\n",
    "    (\"the\", \"real\"): [\"life?\", ...],\n",
    "    (\"real\", \"life?\"): [\"<N>\", ...],\n",
    "    (\"<N>\", \"Nothing\"): [\"really\", ...],\n",
    "    (\"Nothing\", \"really\"): [\"matters\", ...],\n",
    "    (\"really\", \"matters\"): [\"to\", ...],\n",
    "    (\"matters\", \"to\"): [\"me.\", ...],\n",
    "    (\"to\", \"me.\"): [\"<END>\", ...],\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_markov_chain(lyrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - lyrics: a list of strings, where each string represents\n",
    "                the lyrics of one song by an artist.\n",
    "    \n",
    "    Returns:\n",
    "      A dict that maps a tuple of 2 words (\"bigram\") to a list of\n",
    "      words that follow that bigram, representing the Markov\n",
    "      chain trained on the lyrics.\n",
    "    \"\"\"\n",
    "    chain = {(None, \"<START>\"): []}\n",
    "    for lyric in lyrics:\n",
    "        words = lyric.split()\n",
    "        if(words[0] == ''):\n",
    "            continue\n",
    "          \n",
    "        chain[(None, \"<START>\")].append(words[0])\n",
    "        if (\"<START>\", words[0]) in chain:\n",
    "            chain[\"<START>\", words[0]].append(words[1])\n",
    "        else:\n",
    "            chain[\"<START>\", words[0]] = [words[1]]\n",
    "    \n",
    "        for i in range(0,len(words)-1):\n",
    "            first = words[i]\n",
    "            second = words[i+1]\n",
    "             \n",
    "            if i == len(words)-2:\n",
    "                next_word = \"<END>\"\n",
    "            else:\n",
    "                next_word = words[i+2]\n",
    "            \n",
    "            pair = (first, second)\n",
    "            if pair in chain:\n",
    "                chain[pair].append(next_word)\n",
    "            else:\n",
    "                chain[pair] = [next_word]\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N-now', 'I', 'She', 'In', \"I'm\", \"I'm\", 'Like', 'And', 'I', 'All', \"You're\", \"I'm\", 'La', 'Just', 'Well!', \"'As\", 'Oh', 'What', 'I', 'Flashing', 'Uh,', 'I', \"Who's\", 'Bound', 'Real', 'All', 'Yo,', \"I'm\", 'She', 'Lost', 'For', 'You', 'Your', 'Order,', '(Hey', \"It's\", 'What', 'Turn', 'Man', '(Mr.', 'Now', 'I', 'Stadium', 'No', 'Find', 'Are', 'Yeah,', \"I'm\", 'Strange', 'You', 'When', 'One', \"You're\", \"Ain't\", 'Alright,', 'So', 'Human', '(Did', 'Dirt', 'My', '\"We\\'re', 'Let', 'It', \"Talkin'\", 'I', 'You', '1', 'Bittersweet', 'Blazin,', 'La', 'Yeezy', 'I', 'Why', 'While', 'I', 'Just', 'I', 'Feel', 'If', 'It', '\"We\\'re', 'Ooo,', 'No', 'I', 'And', 'Tiimmy,', 'Me', 'Are', 'DJ', 'You']\n"
     ]
    }
   ],
   "source": [
    "# Load the pickled lyrics object that you created in Part 1.\n",
    "import pickle\n",
    "lyrics = pickle.load(open(\"lyrics.pkl\", \"rb\"))\n",
    "\n",
    "# Call the function you wrote above.\n",
    "chain = train_markov_chain(lyrics)\n",
    "\n",
    "# What words tend to start a song (i.e., what words follow the <START> tag?)\n",
    "print(chain[(None, \"<START>\")][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate new lyrics using the Markov chain you constructed above. To do this, we'll begin at the `(None, \"<START>\")` state and randomly sample a word from the list of first words. Then, we'll randomly sample each next word from the list of words that appeared after the current word in the training data. We will continue this until we reach the `\"<END>\"` state. This will give us the complete lyrics of a randomly generated song!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_new_lyrics(chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - chain: a dict representing the Markov chain,\n",
    "               such as one generated by generate_new_lyrics()\n",
    "    \n",
    "    Returns:\n",
    "      A string representing the randomly generated song.\n",
    "    \"\"\"\n",
    "    \n",
    "    # a list for storing the generated words\n",
    "    words = []\n",
    "    # generate the first word\n",
    "    curr_word = random.choice(chain[(None, \"<START>\")])\n",
    "    words.append(curr_word)\n",
    "\n",
    "    p1 = \"<START>\"\n",
    "    p2 = curr_word\n",
    "    while(curr_word != \"<END>\"):\n",
    "        next_word = random.choice(chain[(p1,p2)])\n",
    "        words.append(next_word)\n",
    "        p1 = p2\n",
    "        p2 = next_word\n",
    "        curr_word = next_word\n",
    "        \n",
    "\n",
    "    \n",
    "    # join the words together into a string with line breaks\n",
    "    lyrics = \" \".join(words[:-1])\n",
    "    return \"\\n\".join(lyrics.split(\"<N>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm gon' praise Him, praise Him 'til I'm gone \n",
      " (Good God) \n",
      " The sun is only half way lit, you haven't come out your crib \n",
      " You feel it (We zonin') \n",
      " We deserve, we deserve \n",
      " We gon' get it poppin' \n",
      " We invented rock before the Stones got through \n",
      " We just be \n",
      " Iconic, ironic\n"
     ]
    }
   ],
   "source": [
    "print(generate_new_lyrics(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grader's Comments\n",
    "\n",
    "- \n",
    "- \n",
    "\n",
    "[This question is worth 20 points.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell should only be modified only by a grader.\n",
    "scores = [None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
